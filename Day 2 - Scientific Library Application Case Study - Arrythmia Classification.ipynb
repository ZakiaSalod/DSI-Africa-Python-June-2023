{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d79ea393",
   "metadata": {},
   "source": [
    "# DS-I Africa Python Boot Camp - 22 June 2023\n",
    "## Mandla Gwetu (mvgcollab@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1871fcf2",
   "metadata": {},
   "source": [
    "#### The code in this notebook is adapted from https://github.com/sidhunk/HCAADUMMDLT\n",
    "#### The following changes have been made to the code:\n",
    "* Minor adjustments were made to file paths \n",
    "* Commented out code statements were deleted\n",
    "* Code snippets were separated into notebook code cells\n",
    "\n",
    "#### The dataset was downloaded from https://archive.ics.uci.edu/dataset/5/arrhythmia and file extension changed to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd94844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import pywt\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, AvgPool1D, Flatten, Dense, Dropout, Softmax\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import regularizers\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.layers import LeakyReLU\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Activation, Add, Embedding, Conv1DTranspose, RepeatVector, Softmax, Conv1D, Flatten, UpSampling1D, MaxPooling1D, AveragePooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.python.keras.layers.recurrent import LSTM\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "from keras.models import model_from_json\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "import glob\n",
    "warnings.filterwarnings('ignore')\n",
    "from google.colab import drive\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "import pandas\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0294c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/mntDrive') #, force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0b6e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_column( X, col_num ):\n",
    "\tmean=0.0\n",
    "\tc=0\n",
    "\tfor i in range(0,452):\n",
    "\t\tif(X[i][col_num] !=\"?\"):\t\n",
    "\t\t\tmean=mean+X[i][col_num].astype(float)\n",
    "\t\t\tc=c+1\n",
    "\tmean=mean/c\n",
    "\treturn mean\n",
    "\n",
    "def standard_deviation_column( X, col_num ,mean):\n",
    "\tsd=0.0\n",
    "\tc=0\n",
    "\tfor i in range(0,452):\n",
    "\t\tif(X[i][col_num] !=\"?\"):\t\n",
    "\t\t\tsd=(X[i][col_num].astype(float)-mean)**2\n",
    "\t\t\tc=c+1\n",
    "\tsd=sd/(c-1)\n",
    "\tsd=sd**0.5\n",
    "\treturn sd\n",
    "\n",
    "def convert_strarr_floatarr( arr, X):\n",
    "\tfor i in range(0,452):\n",
    "\t\tfor j in range(0,278):\n",
    "\t\t\tif(arr[i][j]==\"?\"):\t\n",
    "\t\t\t\tX[i][j]=0.0\n",
    "\t\t\telse:\n",
    "\t\t\t\tX[i][j]=arr[i][j].astype(float)\n",
    "\treturn\n",
    "\n",
    "reader=csv.reader(open(\"/mntDrive/MyDrive/DSIWorkshop/DLWorkshop/arrhythmia.csv\",\"r\"),delimiter=\",\")\n",
    "arr=list(reader)\n",
    "arr=np.array(arr)\n",
    "data=np.zeros((452,2))\n",
    "c=0\n",
    "for i in range(0,452):\n",
    "\tfor j in range(0,279):\n",
    "\t\tif(arr[i][j] ==\"?\"):\t\n",
    "\t\t\tdata[c][0]=i\n",
    "\t\t\tdata[c][1]=j\n",
    "\t\t\tc=c+1\n",
    "\n",
    "#majority of the values are missing so delete coulmn 13\n",
    "#find the columns with missing values\t\t\t\n",
    "for i in range(0,c):\n",
    "\tif(data[i][1]!=13):\n",
    "\t\tprint(data[i][0],data[i][1])\n",
    "\n",
    "#remove coulmn 13\n",
    "arr = np.delete(arr,13,1)\n",
    "\n",
    "#create feature matrix\n",
    "X=np.zeros((452,278),dtype=float)\n",
    "convert_strarr_floatarr(arr,X)\n",
    "\n",
    "#create result vector\n",
    "y=np.zeros((452),dtype=int)\n",
    "for i in range(0,452):\n",
    "\ty[i]=arr[i][278].astype(int)\n",
    "print (y)\n",
    "\n",
    "#find the columns with missing values\t\t\t\n",
    "for i in range(0,c):\n",
    "\tif(data[i][1]!=13):\n",
    "\t\tprint(data[i][0],data[i][1])\n",
    "\n",
    "#calculate mean for column 13(initially 14),11,10,12\n",
    "mean=mean_column(X,13)\n",
    "print (\"mean=\"+str(mean))\n",
    "sd=standard_deviation_column(X,13,mean)\n",
    "\n",
    "for i in range(0,452):\n",
    "\tif(arr[i][13]==\"?\"):\n",
    "\t\tval = np.random.normal(mean,sd,1)\n",
    "\t\tprint (val)\n",
    "\t\tX[i][13]=(val).astype(int)\n",
    "\t\tprint (X[i][13])\n",
    "\n",
    "mean=mean_column(X,10)\n",
    "print (\"mean=\"+str(mean))\n",
    "sd=standard_deviation_column(X,10,mean)\n",
    "\n",
    "for i in range(0,452):\n",
    "\tif(arr[i][10]==\"?\"):\n",
    "\t\tval = np.random.normal(mean,sd,1)\n",
    "\t\tprint(val)\n",
    "\t\tX[i][10]=(val).astype(int)\n",
    "\t\tprint (X[i][10])\n",
    "\n",
    "mean=mean_column(X,11)\n",
    "print (\"mean=\"+str(mean))\n",
    "sd=standard_deviation_column(X,11,mean)\n",
    "\n",
    "for i in range(0,452):\n",
    "\tif(arr[i][11]==\"?\"):\n",
    "\t\tval = np.random.normal(mean,sd,1)\n",
    "\t\tprint (val)\n",
    "\t\tX[i][11]=(val).astype(int)\n",
    "\t\tprint (X[i][11])\n",
    "\n",
    "mean=mean_column(X,12)\n",
    "print (\"mean=\"+str(mean))\n",
    "sd=standard_deviation_column(X,12,mean)\n",
    "\n",
    "for i in range(0,452):\n",
    "\tif(arr[i][12]==\"?\"):\n",
    "\t\tval = np.random.normal(mean,sd,1)\n",
    "\t\tprint (val)\n",
    "\t\tX[i][12]=(val).astype(int)\n",
    "\t\tprint (X[i][12])\n",
    "#reduce number of classes\n",
    "for i in range(0,452):\n",
    "\tif (y[i]>=14):\n",
    "\t\ty[i]=y[i]-3\n",
    "\n",
    "np.savetxt(\"feature.csv\", X, fmt='%s', delimiter=\",\")\n",
    "np.savetxt(\"target_output.csv\", y, fmt='%s', delimiter=\",\")\n",
    "\n",
    "def convert_strarr_floatarr( arr, X):\n",
    "\tfor i in range(0,452):\n",
    "\t\tfor j in range(0,278):\n",
    "\t\t\t\tX[i][j]=arr[i][j].astype(float)\n",
    "\treturn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea3c8a7",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcbcadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create feature matrix\n",
    "reader=csv.reader(open(\"feature.csv\",\"r\"),delimiter=\",\")\n",
    "X=list(reader)\n",
    "X=np.array(X)\n",
    "X=X.astype(float)\n",
    "\n",
    "#create result vector\n",
    "reader=csv.reader(open(\"target_output.csv\",\"r\"),delimiter=\",\")\n",
    "Y=list(reader)\n",
    "Y=np.array(Y)\n",
    "Y=Y.astype(int)\n",
    "\n",
    "#applying PCA to get pricipal attributes\n",
    "pca = PCA(n_components=50)\n",
    "X=pca.fit_transform(X)\n",
    "\n",
    "print (pca.explained_variance_ratio_)\n",
    "\n",
    "np.savetxt(\"reduced_features.csv\",X, fmt='%s', delimiter=\",\")\n",
    "\n",
    "readerF=csv.reader(open(\"reduced_features.csv\",\"r\"),delimiter=\",\")\n",
    "X=list(readerF)\n",
    "X=np.array(X)\n",
    "X=X.astype(float)\n",
    "\n",
    "#feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X=sc.fit_transform(X)\n",
    "\n",
    "readerL=csv.reader(open(\"target_output.csv\",\"r\"),delimiter=\",\")\n",
    "Y=list(readerL)\n",
    "Y=np.array(Y)\n",
    "Y=Y.astype(int)\n",
    "\n",
    "#splitting the dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test= train_test_split(X,Y,test_size=0.40, random_state=0)\n",
    "\n",
    "Y_train=np.transpose(Y_train)\n",
    "Y_test=np.transpose(Y_test)\n",
    "\n",
    "#one hot encoding for multiclass (Training labels)\n",
    "one_hot_encoded=list()\n",
    "for value in range (0,Y_train.shape[1]):\n",
    "\tout=list()\n",
    "\tout=[0 for i in range(13)]\n",
    "\tout[Y_train[0][value]-1]=1\n",
    "\tone_hot_encoded.append(out)\n",
    " \n",
    "Y_train=one_hot_encoded\n",
    "Y_train=np.array(Y_train)\n",
    "\n",
    "#one hot encoding for multiclass (Testing labels)\n",
    "one_hot_encoded2=list()\n",
    "for value in range (0,Y_test.shape[1]):\n",
    "\tout2=list()\n",
    "\tout2=[0 for i in range(13)]\n",
    "\tout2[Y_test[0][value]-1]=1\n",
    "\tone_hot_encoded2.append(out2)\n",
    " \n",
    "Y_test=one_hot_encoded2\n",
    "Y_test=np.array(Y_test)\n",
    "\n",
    "print(\"shape of XTrain =\"+str(X_train.shape))\n",
    "print(\"shape of YTrain =\"+str(Y_train.shape))\n",
    "\n",
    "print(\"shape of XTest =\"+str(X_test.shape))\n",
    "print(\"shape of YTest =\"+str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8e259",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da675af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.Sequential()\n",
    "model1.add(tf.keras.layers.LSTM(units = 64, activation='relu', input_shape = (50,1)))\n",
    "model1.add(Dense(13, activation='relu'))\n",
    "model1.add(tf.keras.layers.BatchNormalization())\n",
    "model1.add(Dense(13, activation='softmax'))\n",
    "model1.summary()\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model1.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5882f6b1",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774c84b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = tf.keras.Sequential()\n",
    "model3.add(tf.keras.Input(shape = (50,1)))\n",
    "\n",
    "model3.add(Conv1D(2, kernel_size=2, strides=1, padding='same', activation='relu'))\n",
    "model3.add(MaxPooling1D(pool_size=1, strides=1))\n",
    "\n",
    "model3.add(Conv1D(2, kernel_size=4, strides=1, padding='same', activation='relu'))\n",
    "model3.add(MaxPooling1D(pool_size=1, strides=1))\n",
    "\n",
    "model3.add(Conv1D(2, kernel_size=8, strides=1, padding='same', activation='relu'))\n",
    "model3.add(MaxPooling1D(pool_size=1, strides=1))\n",
    "\n",
    "model3.add(tf.keras.layers.Dropout(.3, noise_shape=None, seed=None))\n",
    "model3.add(tf.keras.layers.Flatten(data_format=None))\n",
    "model3.add(tf.keras.layers.BatchNormalization())\n",
    "model3.add(Dense(13, activation='relu'))\n",
    "model3.add(Dense(13, activation='softmax'))\n",
    "model3.summary()\n",
    "\n",
    "model3.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50b8d85",
   "metadata": {},
   "source": [
    "# Merging all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e97b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedOut = Add()([model1.output, model3.output])\n",
    "newModel = Model([model1.input, model3.input], mergedOut)\n",
    "newModel.summary()\n",
    "\n",
    "newModel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "newModel.fit([X_train, X_train], Y_train, epochs=500, batch_size=20, shuffle=False)\n",
    "\n",
    "preddd = newModel.predict([X_test, X_test])\n",
    "print(preddd.shape)\n",
    "preddd = np.argmax(preddd, axis=1)\n",
    "print(preddd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b94f59",
   "metadata": {},
   "source": [
    "# Final Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713466cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_testNM = np.argmax(Y_test, axis=1)\n",
    "print(max_testNM)\n",
    "\n",
    "Accuracy = accuracy_score(max_testNM, preddd)\n",
    "NAccuracy = Accuracy * 100\n",
    "print('Merged Model Accuracy:', NAccuracy, '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
